{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01855e45",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10436\\216670320.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Extract desired profile information (example: name and job title)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprofile_soup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'inline t-24 t-black t-normal break-words'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mjob_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprofile_soup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'mt1 t-18 t-black t-normal break-words'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome()  # Replace with the path to your ChromeDriver executable\n",
    "\n",
    "# Perform Google search\n",
    "company_name = \"Glenmark\"  # Replace with the desired company name\n",
    "search_query = f'site:linkedin.com/in AND \"Glenmark\" AND (\"Talent Acquisition\" OR \"Human Resource\")'\n",
    "google_url = f\"https://www.google.com/search?q={search_query}\"\n",
    "driver.get('https://www.google.com/search?q=site%3Alinkedin.com%2Fin+AND+%22Glenmark%22+AND+(%22Talent+Acquisition%22+OR+%22Human+Resource%22)&rlz=1C1GCEA_enIN1021IN1021&oq=site%3Alinkedin.com%2Fin+AND+%22Glenmark%22+AND+(%22Talent+Acquisition%22+OR+%22Human+Resource%22)&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIGCAEQRRg60gEHOTMxajBqMagCALACAA&sourceid=chrome&ie=UTF-8')\n",
    "\n",
    "# Extract LinkedIn profile URLs from search results\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "search_results = soup.find_all('div', {'class': 'yuRUbf'})\n",
    "profile_urls = []\n",
    "for result in search_results:\n",
    "    link = result.find('a')['href']\n",
    "    if \"linkedin.com/in\" in link:\n",
    "        profile_urls.append(link)\n",
    "\n",
    "# Visit each LinkedIn profile and extract data\n",
    "profile_data = []\n",
    "for profile_url in profile_urls:\n",
    "    driver.get(profile_url)\n",
    "    profile_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Extract desired profile information (example: name and job title)\n",
    "    name = profile_soup.find('li', {'class': 'inline t-24 t-black t-normal break-words'}).text.strip()\n",
    "    job_title = profile_soup.find('h2', {'class': 'mt1 t-18 t-black t-normal break-words'}).text.strip()\n",
    "    \n",
    "    # Add profile data to the list\n",
    "    profile_data.append({'Name': name, 'Job Title': job_title})\n",
    "\n",
    "# Close the Selenium WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Print the extracted profile data\n",
    "for profile in profile_data:\n",
    "    print(profile)\n",
    "\n",
    "# You can also store the data in a database or a file for further use\n",
    "# For example, to store as CSV\n",
    "import csv\n",
    "\n",
    "csv_filename = 'linkedin_profiles.csv'\n",
    "fieldnames = ['Name', 'Job Title']\n",
    "\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(profile_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f7525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Perform Google search\n",
    "company_name = \"Glenmark\"  # Replace with the desired company name\n",
    "search_query = f'site:linkedin.com/in AND \"Glenmark\" AND \"Talent Acquisition\"'\n",
    "google_url = f\"https://www.google.com/search?q={search_query}\"\n",
    "\n",
    "# Send GET request to Google search page\n",
    "response = requests.get('https://www.google.com/search?q=site%3Alinkedin.com%2Fin+AND+%22Glenmark%22+AND+(%22Talent+Acquisition%22+OR+%22Human+Resource%22)&rlz=1C1GCEA_enIN1021IN1021&oq=site%3Alinkedin.com%2Fin+AND+%22Glenmark%22+AND+(%22Talent+Acquisition%22+OR+%22Human+Resource%22)&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIGCAEQRRg60gEHOTMxajBqMagCALACAA&sourceid=chrome&ie=UTF-8')\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract LinkedIn profile URLs from search results\n",
    "search_results = soup.find_all('div', {'class': 'yuRUbf'})\n",
    "profile_urls = []\n",
    "for result in search_results:\n",
    "    link = result.find('a')['href']\n",
    "    if \"linkedin.com/in\" in link:\n",
    "        profile_urls.append(link)\n",
    "\n",
    "# Visit each LinkedIn profile and extract data\n",
    "profile_data = []\n",
    "for profile_url in profile_urls:\n",
    "    response = requests.get(profile_url)\n",
    "    profile_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract desired profile information (example: name and job title)\n",
    "    name = profile_soup.find('li', {'class': 'inline t-24 t-black t-normal break-words'}).text.strip()\n",
    "    job_title = profile_soup.find('h2', {'class': 'mt1 t-18 t-black t-normal break-words'}).text.strip()\n",
    "    \n",
    "    # Add profile data to the list\n",
    "    profile_data.append({'Name': name, 'Job Title': job_title})\n",
    "\n",
    "# Print the extracted profile data\n",
    "for profile in profile_data:\n",
    "    print(profile)\n",
    "\n",
    "# You can also store the data in a database or a file for further use\n",
    "# For example, to store as CSV\n",
    "import csv\n",
    "\n",
    "csv_filename = 'linkedin_profiles.csv'\n",
    "fieldnames = ['Name', 'Job Title']\n",
    "\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(profile_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7374ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googlesearch-python\n",
      "  Downloading googlesearch-python-1.2.3.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from googlesearch-python) (4.11.1)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from googlesearch-python) (2.28.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2022.9.14)\n",
      "Building wheels for collected packages: googlesearch-python\n",
      "  Building wheel for googlesearch-python (setup.py): started\n",
      "  Building wheel for googlesearch-python (setup.py): finished with status 'done'\n",
      "  Created wheel for googlesearch-python: filename=googlesearch_python-1.2.3-py3-none-any.whl size=4211 sha256=b7adebbea0804d59b65bc8679c4fc9562af1052c55e17f00c3496824bcd1df6a\n",
      "  Stored in directory: c:\\users\\administrator\\appdata\\local\\pip\\cache\\wheels\\be\\78\\a3\\d80e85ac9551489cd9c44f59a0bc5972e79e680ce9cc6055ca\n",
      "Successfully built googlesearch-python\n",
      "Installing collected packages: googlesearch-python\n",
      "Successfully installed googlesearch-python-1.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install googlesearch-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b034b66e",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3Dsite%253Alinkedin.com%252Fin%252BAND%252B%2522Glenmark%2522%252BAND%252B%2522Talent%252BAcquisition%2522%26num%3D4%26hl%3Den%26start%3D0&hl=en&q=EgRnTAjnGN6diqUGIjDBWAyHewEueuulvrjJWbyZXIYTlP12dof85irtfnMiK4RM_ma0IDzWsy1ke_n9X7cyAXJaAUM",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10436\\3016265997.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Extract LinkedIn profile data from search results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprofile_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msearch_results\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\googlesearch\\__init__.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(term, num_results, lang, proxy, advanced, sleep_interval, timeout)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mnum_results\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# Send request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         resp = _req(escaped_term, num_results - start,\n\u001b[0m\u001b[0;32m     55\u001b[0m                     lang, start, proxies, timeout)\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\googlesearch\\__init__.py\u001b[0m in \u001b[0;36m_req\u001b[1;34m(term, results, lang, start, proxies, timeout)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     )\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3Dsite%253Alinkedin.com%252Fin%252BAND%252B%2522Glenmark%2522%252BAND%252B%2522Talent%252BAcquisition%2522%26num%3D4%26hl%3Den%26start%3D0&hl=en&q=EgRnTAjnGN6diqUGIjDBWAyHewEueuulvrjJWbyZXIYTlP12dof85irtfnMiK4RM_ma0IDzWsy1ke_n9X7cyAXJaAUM"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Perform Google search\n",
    "company_name = \"Glenmark\"  # Replace with the desired company name\n",
    "search_query = f'site:linkedin.com/in AND \"Glenmark\" AND \"Talent Acquisition\"'\n",
    "num_results =  2 # Number of search results to retrieve\n",
    "search_results = search(search_query, num_results=num_results)\n",
    "\n",
    "# Extract LinkedIn profile data from search results\n",
    "profile_data = []\n",
    "for url in search_results:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract desired profile information (example: name and job title)\n",
    "    name = soup.find('h1', {'class': 'text-heading-xlarge inline t-24 v-align-middle break-words'}).text.strip()\n",
    "    job_title = soup.find('h2', {'class': 'mt1 t-18 t-black t-normal break-words'}).text.strip()\n",
    "    \n",
    "    # Add profile data to the list\n",
    "    profile_data.append({'Name': name, 'Job Title': job_title})\n",
    "\n",
    "# Print the extracted profile data\n",
    "for profile in profile_data:\n",
    "    print(profile)\n",
    "\n",
    "# You can also store the data in a database or a file for further use\n",
    "# For example, to store as CSV\n",
    "import csv\n",
    "\n",
    "csv_filename = 'linkedin_profiles.csv'\n",
    "fieldnames = ['Name', 'Job Title']\n",
    "\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(profile_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "sleep(5)\n",
    "driver.maximize_window()\n",
    "sleep(5)\n",
    "driver.get(\"https://www.linkedin.com/\")\n",
    "sleep(5)\n",
    "\n",
    "cookies_dict = {}\n",
    "for cookie in driver.get_cookies():\n",
    "        cookies_dict[cookie['name']] = cookie['value']\n",
    "        \n",
    "driver.close()\n",
    "\n",
    "resp = requests.get(\"https://www.linkedin.com/company/twitter\",\n",
    "                     cookies=cookies_dict,\n",
    "                     headers={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36',\n",
    "                              'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "                                                   'accept-encoding': 'gzip, deflate, br',\n",
    "                                                   'accept-language': 'en-US,en;q=0.9',\n",
    "                                                   'upgrade-insecure-requests': '1',\n",
    "                                                   'scheme': 'https'})\n",
    "        \n",
    "html = resp.text\n",
    "\n",
    "\n",
    "HtmlPath = \"D:/linkedin_pages/1.html\"\n",
    "page_fun = open(HtmlPath,'w',encoding='utf-8')\n",
    "page_fun.write(html)\n",
    "page_fun.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc9e6f4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'linkedin_profiles.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10436\\2470156186.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m csv_writer = csv.writer(csv_file)'''\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mcsv_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfieldnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mcsv_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteheader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'linkedin_profiles.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Set up the headers to mimic a web browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Company name and desired job title (e.g., \"HR\")\n",
    "company_name = \"Glenmark\"\n",
    "job_title = \"Talent Acquisition\"\n",
    "\n",
    "# Google search query to find LinkedIn profiles from the specific company and job title\n",
    "search_query = f'site:linkedin.com/in \"Glenmark\" \"Talent Acquisition\"'\n",
    "google_url = f\"https://www.google.com/search?q={search_query}\"\n",
    "\n",
    "# Send a GET request to Google search page\n",
    "response = requests.get(google_url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Extract LinkedIn profile URLs from search results\n",
    "search_results = soup.find_all(\"div\", {\"class\": \"yuRUbf\"})\n",
    "profile_urls = []\n",
    "for result in search_results:\n",
    "    link = result.find(\"a\")[\"href\"]\n",
    "    if \"linkedin.com/in\" in link:\n",
    "        profile_urls.append(link)\n",
    "\n",
    "# List to store the profile data\n",
    "profile_data = []\n",
    "\n",
    "# Scrape each profile\n",
    "for profile_url in profile_urls:\n",
    "    # Send a GET request to the profile URL with headers\n",
    "    response = requests.get(profile_url, headers=headers)\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Extract desired profile information\n",
    "    name_element = soup.find(\"li\", {\"class\": \"inline t-24 t-black t-normal break-words\"})\n",
    "    name = name_element.text.strip() if name_element else \"\"\n",
    "    \n",
    "    headline_element = soup.find(\"h2\", {\"class\": \"mt1 t-18 t-black t-normal break-words\"})\n",
    "    headline = headline_element.text.strip() if headline_element else \"\"\n",
    "    \n",
    "    # Add profile data to the list\n",
    "    profile_data.append({\"Name\": name, \"Headline\": headline})\n",
    "\n",
    "# Save profile data to a CSV file\n",
    "csv_filename = \"linkedin_profiles.csv\"\n",
    "fieldnames = [\"Name\", \"Headline\"]\n",
    "\n",
    "'''csv_file = open('Naukri_scrape_D_C_trial8.csv', 'a', encoding=\"utf-8\", newline='')\n",
    "csv_writer = csv.writer(csv_file)'''\n",
    "\n",
    "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    csv_writer = csv.writer(file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(profile_data)\n",
    "\n",
    "print(f\"Scraped profiles from {company_name} with job title '{job_title}' saved to '{csv_filename}' successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1915bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting linkedin_scraper\n",
      "  Downloading linkedin_scraper-2.11.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: selenium in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from linkedin_scraper) (4.10.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from linkedin_scraper) (4.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from linkedin_scraper) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests->linkedin_scraper) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests->linkedin_scraper) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests->linkedin_scraper) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests->linkedin_scraper) (2022.9.14)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from selenium->linkedin_scraper) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from selenium->linkedin_scraper) (0.10.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (1.1.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (1.10)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (1.15.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (21.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium->linkedin_scraper) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from urllib3<1.27,>=1.21.1->requests->linkedin_scraper) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium->linkedin_scraper) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->linkedin_scraper) (0.14.0)\n",
      "Installing collected packages: linkedin_scraper\n",
      "Successfully installed linkedin_scraper-2.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install linkedin_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e30c6c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\administrator\\anaconda3\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from selenium) (0.10.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: outcome in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99c7ef7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting objects\n",
      "  Downloading objects-0.3.1.tar.gz (1.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: objects\n",
      "  Building wheel for objects (setup.py): started\n",
      "  Building wheel for objects (setup.py): finished with status 'done'\n",
      "  Created wheel for objects: filename=objects-0.3.1-py3-none-any.whl size=2151 sha256=3748f94283294e54caf3fdeec1cbc6001609560b988ed533bd395f5947ee4d8b\n",
      "  Stored in directory: c:\\users\\administrator\\appdata\\local\\pip\\cache\\wheels\\4e\\88\\5c\\b4db10bcf303cd453046b9de2a51f9e812bf7984963e55827d\n",
      "Successfully built objects\n",
      "Installing collected packages: objects\n",
      "Successfully installed objects-0.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b839929d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10436\\643779739.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexpected_conditions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mEC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNoSuchElementException\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mobjects\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExperience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEducation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mScraper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInterest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAccomplishment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mContact\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlinkedin_scraper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mselectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from .objects import Experience, Education, Scraper, Interest, Accomplishment, Contact\n",
    "import os\n",
    "from linkedin_scraper import selectors\n",
    "\n",
    "\n",
    "class Person(Scraper):\n",
    "\n",
    "    __TOP_CARD = \"pv-top-card\"\n",
    "    __WAIT_FOR_ELEMENT_TIMEOUT = 5\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        linkedin_url=None,\n",
    "        name=None,\n",
    "        about=None,\n",
    "        experiences=None,\n",
    "        educations=None,\n",
    "        interests=None,\n",
    "        accomplishments=None,\n",
    "        company=None,\n",
    "        job_title=None,\n",
    "        contacts=None,\n",
    "        driver=None,\n",
    "        get=True,\n",
    "        scrape=True,\n",
    "        close_on_complete=True,\n",
    "        time_to_wait_after_login=0,\n",
    "    ):\n",
    "        self.linkedin_url = linkedin_url\n",
    "        self.name = name\n",
    "        self.about = about or []\n",
    "        self.experiences = experiences or []\n",
    "        self.educations = educations or []\n",
    "        self.interests = interests or []\n",
    "        self.accomplishments = accomplishments or []\n",
    "        self.also_viewed_urls = []\n",
    "        self.contacts = contacts or []\n",
    "\n",
    "        if driver is None:\n",
    "            try:\n",
    "                if os.getenv(\"CHROMEDRIVER\") == None:\n",
    "                    driver_path = os.path.join(\n",
    "                        os.path.dirname(__file__), \"drivers/chromedriver\"\n",
    "                    )\n",
    "                else:\n",
    "                    driver_path = os.getenv(\"CHROMEDRIVER\")\n",
    "\n",
    "                driver = webdriver.Chrome(driver_path)\n",
    "            except:\n",
    "                driver = webdriver.Chrome()\n",
    "\n",
    "        if get:\n",
    "            driver.get(linkedin_url)\n",
    "\n",
    "        self.driver = driver\n",
    "\n",
    "        if scrape:\n",
    "            self.scrape(close_on_complete)\n",
    "\n",
    "    def add_about(self, about):\n",
    "        self.about.append(about)\n",
    "\n",
    "    def add_experience(self, experience):\n",
    "        self.experiences.append(experience)\n",
    "\n",
    "    def add_education(self, education):\n",
    "        self.educations.append(education)\n",
    "\n",
    "    def add_interest(self, interest):\n",
    "        self.interests.append(interest)\n",
    "\n",
    "    def add_accomplishment(self, accomplishment):\n",
    "        self.accomplishments.append(accomplishment)\n",
    "\n",
    "    def add_location(self, location):\n",
    "        self.location = location\n",
    "\n",
    "    def add_contact(self, contact):\n",
    "        self.contacts.append(contact)\n",
    "\n",
    "    def scrape(self, close_on_complete=True):\n",
    "        if self.is_signed_in():\n",
    "            self.scrape_logged_in(close_on_complete=close_on_complete)\n",
    "        else:\n",
    "            print(\"you are not logged in!\")\n",
    "\n",
    "    def _click_see_more_by_class_name(self, class_name):\n",
    "        try:\n",
    "            _ = WebDriverWait(self.driver, self.__WAIT_FOR_ELEMENT_TIMEOUT).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, class_name))\n",
    "            )\n",
    "            div = self.driver.find_element(By.CLASS_NAME, class_name)\n",
    "            div.find_element(By.TAG_NAME, \"button\").click()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    def is_open_to_work(self):\n",
    "        try:\n",
    "            return \"#OPEN_TO_WORK\" in self.driver.find_element(By.CLASS_NAME,\"pv-top-card-profile-picture\").find_element(By.TAG_NAME,\"img\").get_attribute(\"title\")\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def get_experiences(self):\n",
    "        url = os.path.join(self.linkedin_url, \"details/experience\")\n",
    "        self.driver.get(url)\n",
    "        self.focus()\n",
    "        main = self.wait_for_element_to_load(by=By.TAG_NAME, name=\"main\")\n",
    "        self.scroll_to_half()\n",
    "        self.scroll_to_bottom()\n",
    "        main_list = self.wait_for_element_to_load(name=\"pvs-list\", base=main)\n",
    "        for position in main_list.find_elements(By.XPATH,\"li\"):\n",
    "            position = position.find_element(By.CLASS_NAME,\"pvs-entity\")\n",
    "            company_logo_elem, position_details = position.find_elements(By.XPATH,\"*\")\n",
    "\n",
    "            # company elem\n",
    "            company_linkedin_url = company_logo_elem.find_element(By.XPATH,\"*\").get_attribute(\"href\")\n",
    "\n",
    "            # position details\n",
    "            position_details_list = position_details.find_elements(By.XPATH,\"*\")\n",
    "            position_summary_details = position_details_list[0] if len(position_details_list) > 0 else None\n",
    "            position_summary_text = position_details_list[1] if len(position_details_list) > 1 else None\n",
    "            outer_positions = position_summary_details.find_element(By.XPATH,\"*\").find_elements(By.XPATH,\"*\")\n",
    "\n",
    "            if len(outer_positions) == 4:\n",
    "                position_title = outer_positions[0].find_element(By.TAG_NAME,\"span\").find_element(By.TAG_NAME,\"span\").text\n",
    "                company = outer_positions[1].find_element(By.TAG_NAME,\"span\").text\n",
    "                work_times = outer_positions[2].find_element(By.TAG_NAME,\"span\").text\n",
    "                location = outer_positions[3].find_element(By.TAG_NAME,\"span\").text\n",
    "            elif len(outer_positions) == 3:\n",
    "                if \"·\" in outer_positions[2].text:\n",
    "                    position_title = outer_positions[0].find_element(By.TAG_NAME,\"span\").find_element(By.TAG_NAME,\"span\").text\n",
    "                    company = outer_positions[1].find_element(By.TAG_NAME,\"span\").text\n",
    "                    work_times = outer_positions[2].find_element(By.TAG_NAME,\"span\").text\n",
    "                    location = \"\"\n",
    "                else:\n",
    "                    position_title = \"\"\n",
    "                    company = outer_positions[0].find_element(By.TAG_NAME,\"span\").find_element(By.TAG_NAME,\"span\").text\n",
    "                    work_times = outer_positions[1].find_element(By.TAG_NAME,\"span\").text\n",
    "                    location = outer_positions[2].find_element(By.TAG_NAME,\"span\").text\n",
    "\n",
    "            times = work_times.split(\"·\")[0].strip() if work_times else \"\"\n",
    "            duration = work_times.split(\"·\")[1].strip() if len(work_times.split(\"·\")) > 1 else None\n",
    "\n",
    "            from_date = \" \".join(times.split(\" \")[:2]) if times else \"\"\n",
    "            to_date = \" \".join(times.split(\" \")[3:]) if times else \"\"\n",
    "\n",
    "            if position_summary_text and len(position_summary_text.find_element(By.CLASS_NAME,\"pvs-list\").find_element(By.CLASS_NAME,\"pvs-list\").find_elements(By.XPATH,\"li\")) > 1:\n",
    "                descriptions = position_summary_text.find_element(By.CLASS_NAME,\"pvs-list\").find_element(By.CLASS_NAME,\"pvs-list\").find_elements(By.XPATH,\"li\")\n",
    "                for description in descriptions:\n",
    "                    res = description.find_element(By.TAG_NAME,\"a\").find_elements(By.XPATH,\"*\")\n",
    "                    position_title_elem = res[0] if len(res) > 0 else None\n",
    "                    work_times_elem = res[1] if len(res) > 1 else None\n",
    "                    location_elem = res[2] if len(res) > 2 else None\n",
    "\n",
    "\n",
    "                    location = location_elem.find_element(By.XPATH,\"*\").text if location_elem else None\n",
    "                    position_title = position_title_elem.find_element(By.XPATH,\"*\").find_element(By.TAG_NAME,\"*\").text if position_title_elem else \"\"\n",
    "                    work_times = work_times_elem.find_element(By.XPATH,\"*\").text if work_times_elem else \"\"\n",
    "                    times = work_times.split(\"·\")[0].strip() if work_times else \"\"\n",
    "                    duration = work_times.split(\"·\")[1].strip() if len(work_times.split(\"·\")) > 1 else None\n",
    "                    from_date = \" \".join(times.split(\" \")[:2]) if times else \"\"\n",
    "                    to_date = \" \".join(times.split(\" \")[3:]) if times else \"\"\n",
    "\n",
    "                    experience = Experience(\n",
    "                        position_title=position_title,\n",
    "                        from_date=from_date,\n",
    "                        to_date=to_date,\n",
    "                        duration=duration,\n",
    "                        location=location,\n",
    "                        description=description,\n",
    "                        institution_name=company,\n",
    "                        linkedin_url=company_linkedin_url\n",
    "                    )\n",
    "                    self.add_experience(experience)\n",
    "            else:\n",
    "                description = position_summary_text.text if position_summary_text else \"\"\n",
    "\n",
    "                experience = Experience(\n",
    "                    position_title=position_title,\n",
    "                    from_date=from_date,\n",
    "                    to_date=to_date,\n",
    "                    duration=duration,\n",
    "                    location=location,\n",
    "                    description=description,\n",
    "                    institution_name=company,\n",
    "                    linkedin_url=company_linkedin_url\n",
    "                )\n",
    "                self.add_experience(experience)\n",
    "\n",
    "    def get_educations(self):\n",
    "        url = os.path.join(self.linkedin_url, \"details/education\")\n",
    "        self.driver.get(url)\n",
    "        self.focus()\n",
    "        main = self.wait_for_element_to_load(by=By.TAG_NAME, name=\"main\")\n",
    "        self.scroll_to_half()\n",
    "        self.scroll_to_bottom()\n",
    "        main_list = self.wait_for_element_to_load(name=\"pvs-list\", base=main)\n",
    "        for position in main_list.find_elements(By.CLASS_NAME,\"pvs-entity\"):\n",
    "            institution_logo_elem, position_details = position.find_elements(By.XPATH,\"*\")\n",
    "\n",
    "            # company elem\n",
    "            institution_linkedin_url = institution_logo_elem.find_element(By.XPATH,\"*\").get_attribute(\"href\")\n",
    "\n",
    "            # position details\n",
    "            position_details_list = position_details.find_elements(By.XPATH,\"*\")\n",
    "            position_summary_details = position_details_list[0] if len(position_details_list) > 0 else None\n",
    "            position_summary_text = position_details_list[1] if len(position_details_list) > 1 else None\n",
    "            outer_positions = position_summary_details.find_element(By.XPATH,\"*\").find_elements(By.XPATH,\"*\")\n",
    "\n",
    "            institution_name = outer_positions[0].find_element(By.TAG_NAME,\"span\").find_element(By.TAG_NAME,\"span\").text\n",
    "            degree = outer_positions[1].find_element(By.TAG_NAME,\"span\").text\n",
    "\n",
    "            if len(outer_positions) > 2:\n",
    "                times = outer_positions[2].find_element(By.TAG_NAME,\"span\").text\n",
    "\n",
    "                from_date = \" \".join(times.split(\" \")[:2])\n",
    "                to_date = \" \".join(times.split(\" \")[3:])\n",
    "            else:\n",
    "                from_date = None\n",
    "                to_date = None\n",
    "\n",
    "\n",
    "\n",
    "            description = position_summary_text.text if position_summary_text else \"\"\n",
    "\n",
    "            education = Education(\n",
    "                from_date=from_date,\n",
    "                to_date=to_date,\n",
    "                description=description,\n",
    "                degree=degree,\n",
    "                institution_name=institution_name,\n",
    "                linkedin_url=institution_linkedin_url\n",
    "            )\n",
    "            self.add_education(education)\n",
    "\n",
    "    def get_name_and_location(self):\n",
    "        top_panels = self.driver.find_elements(By.CLASS_NAME,\"pv-text-details__left-panel\")\n",
    "        self.name = top_panels[0].find_elements(By.XPATH,\"*\")[0].text\n",
    "        self.location = top_panels[1].find_element(By.TAG_NAME,\"span\").text\n",
    "\n",
    "\n",
    "    def get_about(self):\n",
    "        try:\n",
    "            about = self.driver.find_element(By.ID,\"about\").find_element(By.XPATH,\"..\").find_element(By.CLASS_NAME,\"display-flex\").text\n",
    "        except NoSuchElementException :\n",
    "            about=None\n",
    "        self.about = about\n",
    "\n",
    "    def scrape_logged_in(self, close_on_complete=True):\n",
    "        driver = self.driver\n",
    "        duration = None\n",
    "\n",
    "        root = WebDriverWait(driver, self.__WAIT_FOR_ELEMENT_TIMEOUT).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (\n",
    "                    By.CLASS_NAME,\n",
    "                    self.__TOP_CARD,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.focus()\n",
    "        self.wait(5)\n",
    "\n",
    "        # get name and location\n",
    "        self.get_name_and_location()\n",
    "\n",
    "        self.open_to_work = self.is_open_to_work()\n",
    "\n",
    "        # get about\n",
    "        self.get_about()\n",
    "        driver.execute_script(\n",
    "            \"window.scrollTo(0, Math.ceil(document.body.scrollHeight/2));\"\n",
    "        )\n",
    "        driver.execute_script(\n",
    "            \"window.scrollTo(0, Math.ceil(document.body.scrollHeight/1.5));\"\n",
    "        )\n",
    "\n",
    "        # get experience\n",
    "        self.get_experiences()\n",
    "\n",
    "        # get education\n",
    "        self.get_educations()\n",
    "\n",
    "        driver.get(self.linkedin_url)\n",
    "\n",
    "        # get interest\n",
    "        try:\n",
    "\n",
    "            _ = WebDriverWait(driver, self.__WAIT_FOR_ELEMENT_TIMEOUT).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (\n",
    "                        By.XPATH,\n",
    "                        \"//*[@class='pv-profile-section pv-interests-section artdeco-container-card artdeco-card ember-view']\",\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            interestContainer = driver.find_element(By.XPATH,\n",
    "                \"//*[@class='pv-profile-section pv-interests-section artdeco-container-card artdeco-card ember-view']\"\n",
    "            )\n",
    "            for interestElement in interestContainer.find_elements(By.XPATH, \n",
    "                \"//*[@class='pv-interest-entity pv-profile-section__card-item ember-view']\"\n",
    "            ):\n",
    "                interest = Interest(\n",
    "                    interestElement.find_element(By.TAG_NAME, \"h3\").text.strip()\n",
    "                )\n",
    "                self.add_interest(interest)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # get accomplishment\n",
    "        try:\n",
    "            _ = WebDriverWait(driver, self.__WAIT_FOR_ELEMENT_TIMEOUT).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (\n",
    "                        By.XPATH,\n",
    "                        \"//*[@class='pv-profile-section pv-accomplishments-section artdeco-container-card artdeco-card ember-view']\",\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            acc = driver.find_element(By.XPATH,\n",
    "                \"//*[@class='pv-profile-section pv-accomplishments-section artdeco-container-card artdeco-card ember-view']\"\n",
    "            )\n",
    "            for block in acc.find_elements(By.XPATH, \n",
    "                \"//div[@class='pv-accomplishments-block__content break-words']\"\n",
    "            ):\n",
    "                category = block.find_element(By.TAG_NAME, \"h3\")\n",
    "                for title in block.find_element(By.TAG_NAME, \n",
    "                    \"ul\"\n",
    "                ).find_elements(By.TAG_NAME, \"li\"):\n",
    "                    accomplishment = Accomplishment(category.text, title.text)\n",
    "                    self.add_accomplishment(accomplishment)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # get connections\n",
    "        try:\n",
    "            driver.get(\"https://www.linkedin.com/mynetwork/invite-connect/connections/\")\n",
    "            _ = WebDriverWait(driver, self.__WAIT_FOR_ELEMENT_TIMEOUT).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"mn-connections\"))\n",
    "            )\n",
    "            connections = driver.find_element(By.CLASS_NAME, \"mn-connections\")\n",
    "            if connections is not None:\n",
    "                for conn in connections.find_elements(By.CLASS_NAME, \"mn-connection-card\"):\n",
    "                    anchor = conn.find_element(By.CLASS_NAME, \"mn-connection-card__link\")\n",
    "                    url = anchor.get_attribute(\"href\")\n",
    "                    name = conn.find_element(By.CLASS_NAME, \"mn-connection-card__details\").find_element(By.CLASS_NAME, \"mn-connection-card__name\").text.strip()\n",
    "                    occupation = conn.find_element(By.CLASS_NAME, \"mn-connection-card__details\").find_element(By.CLASS_NAME, \"mn-connection-card__occupation\").text.strip()\n",
    "\n",
    "                    contact = Contact(name=name, occupation=occupation, url=url)\n",
    "                    self.add_contact(contact)\n",
    "        except:\n",
    "            connections = None\n",
    "\n",
    "        if close_on_complete:\n",
    "            driver.quit()\n",
    "\n",
    "    @property\n",
    "    def company(self):\n",
    "        if self.experiences:\n",
    "            return (\n",
    "                self.experiences[0].institution_name\n",
    "                if self.experiences[0].institution_name\n",
    "                else None\n",
    "            )\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def job_title(self):\n",
    "        if self.experiences:\n",
    "            return (\n",
    "                self.experiences[0].position_title\n",
    "                if self.experiences[0].position_title\n",
    "                else None\n",
    "            )\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<Person {name}\\n\\nAbout\\n{about}\\n\\nExperience\\n{exp}\\n\\nEducation\\n{edu}\\n\\nInterest\\n{int}\\n\\nAccomplishments\\n{acc}\\n\\nContacts\\n{conn}>\".format(\n",
    "            name=self.name,\n",
    "            about=self.about,\n",
    "            exp=self.experiences,\n",
    "            edu=self.educations,\n",
    "            int=self.interests,\n",
    "            acc=self.accomplishments,\n",
    "            conn=self.contacts,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc87ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
